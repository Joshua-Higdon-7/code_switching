{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4438be41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\valer\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\valer\\anaconda3\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\valer\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\valer\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\valer\\anaconda3\\lib\\site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\valer\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\valer\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\valer\\anaconda3\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\valer\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\valer\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\valer\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\valer\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\valer\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\valer\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: codeswitch in c:\\users\\valer\\anaconda3\\lib\\site-packages (1.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\valer\\anaconda3\\lib\\site-packages (from codeswitch) (4.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\valer\\anaconda3\\lib\\site-packages (from transformers->codeswitch) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from transformers->codeswitch) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from transformers->codeswitch) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from transformers->codeswitch) (2022.3.15)\n",
      "Requirement already satisfied: requests in c:\\users\\valer\\anaconda3\\lib\\site-packages (from transformers->codeswitch) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from transformers->codeswitch) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from transformers->codeswitch) (0.13.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from transformers->codeswitch) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from transformers->codeswitch) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers->codeswitch) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers->codeswitch) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\valer\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers->codeswitch) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from requests->transformers->codeswitch) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from requests->transformers->codeswitch) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from requests->transformers->codeswitch) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\valer\\anaconda3\\lib\\site-packages (from requests->transformers->codeswitch) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "# import appropriate packages/libraries \n",
    "!pip3 install torch torchvision torchaudio\n",
    "!pip install nltk \n",
    "!pip install codeswitch\n",
    "import numpy as np\n",
    "import pandas as pan\n",
    "from collections import Counter\n",
    "import torch\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b37279fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "slayta = pan.read_csv(\"herring1.csv\", sep = ',', encoding = \"unicode_escape\")\n",
    "\n",
    "# remove items in stopword list\n",
    "words = list(slayta ['surface'])\n",
    "\n",
    "# import stopwords lists\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "stop_words = [\"oh\", \"ok\", \"mmhm\", \"er\", \"ah\", \"ay\", \"um\", \"uhhuh\", \"wow\", \"huh\", \"\"]\n",
    "total = punct + stop_words\n",
    "\n",
    "super_slay = []\n",
    "for ele in words:\n",
    "    if ele in total:\n",
    "        super_slay.append('1')\n",
    "    else:\n",
    "        super_slay.append('0')\n",
    "        \n",
    "# add column to the pandas data frame of whether the word should not be included\n",
    "# the slayta.insert line has a note because it kept trying to add the same column\n",
    "# every time. If you are running for the first time, take out the note!\n",
    "inclusion = pan.Series(super_slay)\n",
    "slayta.insert(6, 'Inclusion', inclusion)\n",
    "options = ['0']\n",
    "slayta = slayta.loc[slayta['Inclusion'].isin(options)]\n",
    "\n",
    "# make a list of POS tags \n",
    "slayta = slayta[slayta.auto != \"unk\"]\n",
    "slayta['auto'] = slayta['auto'].fillna(\"NaN\")\n",
    "column_tags = list(slayta[\"auto\"])\n",
    "weird_tags = [ele.split(\",\") for ele in column_tags]\n",
    "\n",
    "# get the tags and get specific tags if encountered with contractions\n",
    "cute_list = [item.split(\".\") for sublist in weird_tags for item in sublist]\n",
    "final_list = [sublist[0] if len(sublist) <= 1\n",
    "          else 'N' if 'name' in sublist \n",
    "          else 'V' if 'SV' in sublist\n",
    "          else sublist[1] if 'V' not in sublist and '+BE' not in sublist \n",
    "          else 'V' if 'V' in sublist and \"PRON\" not in sublist\n",
    "          else sublist[1] + '+V' for sublist in cute_list]\n",
    "\n",
    "# convert the list of POS tags to a pandas series, add the pandas series to the existing data frame\n",
    "slayta.insert(5, 'correct_tags', final_list)\n",
    "\n",
    "# create a data frame with the words and tags grouped by utterance id\n",
    "slay_group = slayta.groupby('utterance_id').agg(lambda x: x.tolist())\n",
    "\n",
    "# count and print out the use of english and spanish\n",
    "slay_eng = (slayta[\"langid\"] == 'eng').sum()\n",
    "slay_spa = (slayta[\"langid\"] == 'spa').sum()\n",
    "\n",
    "# get a list of sentences\n",
    "sent_array = np.array(slay_group['surface'])\n",
    "sentences = [list(i) for i in sent_array]\n",
    "\n",
    "# get a list of tags\n",
    "tag_array = np.array(slay_group['correct_tags'])\n",
    "tags = [list(i) for i in tag_array]\n",
    "\n",
    "# get a list of langauges for the utterances\n",
    "lang_array = np.array(slay_group['langid'])\n",
    "languages = [list(i) for i in lang_array]\n",
    "\n",
    "# make a list of word bigrams at the utterance level \n",
    "new_sentences = []\n",
    "for i in sentences:\n",
    "    new_sentences.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a list of tag bigrams at the utterance level\n",
    "new_tags = []\n",
    "for i in tags:\n",
    "    new_tags.append(list(nltk.bigrams(i)))\n",
    "    \n",
    "new_languages = []\n",
    "for i in languages:\n",
    "    new_languages.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a merged list of the bigrams with their respective tags and language id\n",
    "merged_list = [tup_A + tup_B + tup_C for sublst_A, sublst_B, sublst_C in zip(new_sentences, new_tags, new_languages)\n",
    "               for tup_A, tup_B, tup_C in zip(sublst_A, sublst_B, sublst_C)]\n",
    "\n",
    "# get only the instances where is code switching by checking the language id \n",
    "code_switching = [item for item in merged_list if item[4] == 'spa' and item[5] == 'eng' \n",
    "                  or item[4] == 'eng' and item[5] == 'spa']\n",
    "\n",
    "# get only the PoS tags\n",
    "first_tag = [x[2] for x in code_switching]\n",
    "second_tag = [x[3] for x in code_switching]\n",
    "two_tags = list(zip(first_tag, second_tag))\n",
    "\n",
    "# get the most common tags \n",
    "a = Counter(tuple(i) for i in two_tags).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53be0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "slayta = pan.read_csv(\"herring2.csv\", sep = ',', encoding = \"unicode_escape\")\n",
    "\n",
    "# remove items in stopword list\n",
    "words = list(slayta ['surface'])\n",
    "\n",
    "# import stopwords lists\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "stop_words = [\"oh\", \"ok\", \"mmhm\", \"er\", \"ah\", \"ay\", \"um\", \"uhhuh\", \"wow\", \"huh\", \"\"]\n",
    "total = punct + stop_words\n",
    "\n",
    "super_slay = []\n",
    "for ele in words:\n",
    "    if ele in total:\n",
    "        super_slay.append('1')\n",
    "    else:\n",
    "        super_slay.append('0')\n",
    "        \n",
    "# add column to the pandas data frame of whether the word should not be included\n",
    "# the slayta.insert line has a note because it kept trying to add the same column\n",
    "# every time. If you are running for the first time, take out the note!\n",
    "inclusion = pan.Series(super_slay)\n",
    "slayta.insert(6, 'Inclusion', inclusion)\n",
    "options = ['0']\n",
    "slayta = slayta.loc[slayta['Inclusion'].isin(options)]\n",
    "\n",
    "# make a list of POS tags \n",
    "slayta = slayta[slayta.auto != \"unk\"]\n",
    "slayta['auto'] = slayta['auto'].fillna(\"NaN\")\n",
    "column_tags = list(slayta[\"auto\"])\n",
    "weird_tags = [ele.split(\",\") for ele in column_tags]\n",
    "\n",
    "# get the tags and get specific tags if encountered with contractions\n",
    "cute_list = [item.split(\".\") for sublist in weird_tags for item in sublist]\n",
    "final_list = [sublist[0] if len(sublist) <= 1\n",
    "          else 'N' if 'name' in sublist \n",
    "          else 'V' if 'SV' in sublist\n",
    "          else sublist[1] if 'V' not in sublist and '+BE' not in sublist \n",
    "          else 'V' if 'V' in sublist and \"PRON\" not in sublist\n",
    "          else sublist[1] + '+V' for sublist in cute_list]\n",
    "\n",
    "# convert the list of POS tags to a pandas series, add the pandas series to the existing data frame\n",
    "slayta.insert(5, 'correct_tags', final_list)\n",
    "\n",
    "# create a data frame with the words and tags grouped by utterance id\n",
    "slay_group = slayta.groupby('utterance_id').agg(lambda x: x.tolist())\n",
    "\n",
    "# count and print out the use of english and spanish\n",
    "slay_eng = (slayta[\"langid\"] == 'eng').sum()\n",
    "slay_spa = (slayta[\"langid\"] == 'spa').sum()\n",
    "\n",
    "# get a list of sentences\n",
    "sent_array = np.array(slay_group['surface'])\n",
    "sentences = [list(i) for i in sent_array]\n",
    "\n",
    "# get a list of tags\n",
    "tag_array = np.array(slay_group['correct_tags'])\n",
    "tags = [list(i) for i in tag_array]\n",
    "\n",
    "# get a list of langauges for the utterances\n",
    "lang_array = np.array(slay_group['langid'])\n",
    "languages = [list(i) for i in lang_array]\n",
    "\n",
    "# make a list of word bigrams at the utterance level \n",
    "new_sentences = []\n",
    "for i in sentences:\n",
    "    new_sentences.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a list of tag bigrams at the utterance level\n",
    "new_tags = []\n",
    "for i in tags:\n",
    "    new_tags.append(list(nltk.bigrams(i)))\n",
    "    \n",
    "new_languages = []\n",
    "for i in languages:\n",
    "    new_languages.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a merged list of the bigrams with their respective tags and language id\n",
    "merged_list = [tup_A + tup_B + tup_C for sublst_A, sublst_B, sublst_C in zip(new_sentences, new_tags, new_languages)\n",
    "               for tup_A, tup_B, tup_C in zip(sublst_A, sublst_B, sublst_C)]\n",
    "\n",
    "# get only the instances where is code switching by checking the language id \n",
    "code_switching = [item for item in merged_list if item[4] == 'spa' and item[5] == 'eng' \n",
    "                  or item[4] == 'eng' and item[5] == 'spa']\n",
    "\n",
    "# get only the PoS tags\n",
    "first_tag = [x[2] for x in code_switching]\n",
    "second_tag = [x[3] for x in code_switching]\n",
    "two_tags = list(zip(first_tag, second_tag))\n",
    "\n",
    "# get the most common tags \n",
    "b = Counter(tuple(i) for i in two_tags).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de17c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "slayta = pan.read_csv(\"herring3.csv\", sep = ',', encoding = \"unicode_escape\")\n",
    "\n",
    "# remove items in stopword list\n",
    "words = list(slayta ['surface'])\n",
    "\n",
    "# import stopwords lists\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "stop_words = [\"oh\", \"ok\", \"mmhm\", \"er\", \"ah\", \"ay\", \"um\", \"uhhuh\", \"wow\", \"huh\", \"\"]\n",
    "total = punct + stop_words\n",
    "\n",
    "super_slay = []\n",
    "for ele in words:\n",
    "    if ele in total:\n",
    "        super_slay.append('1')\n",
    "    else:\n",
    "        super_slay.append('0')\n",
    "        \n",
    "# add column to the pandas data frame of whether the word should not be included\n",
    "# the slayta.insert line has a note because it kept trying to add the same column\n",
    "# every time. If you are running for the first time, take out the note!\n",
    "inclusion = pan.Series(super_slay)\n",
    "slayta.insert(6, 'Inclusion', inclusion)\n",
    "options = ['0']\n",
    "slayta = slayta.loc[slayta['Inclusion'].isin(options)]\n",
    "\n",
    "# make a list of POS tags \n",
    "slayta = slayta[slayta.auto != \"unk\"]\n",
    "slayta['auto'] = slayta['auto'].fillna(\"NaN\")\n",
    "column_tags = list(slayta[\"auto\"])\n",
    "weird_tags = [ele.split(\",\") for ele in column_tags]\n",
    "\n",
    "# get the tags and get specific tags if encountered with contractions\n",
    "cute_list = [item.split(\".\") for sublist in weird_tags for item in sublist]\n",
    "final_list = [sublist[0] if len(sublist) <= 1\n",
    "          else 'N' if 'name' in sublist \n",
    "          else 'V' if 'SV' in sublist\n",
    "          else sublist[1] if 'V' not in sublist and '+BE' not in sublist \n",
    "          else 'V' if 'V' in sublist and \"PRON\" not in sublist\n",
    "          else sublist[1] + '+V' for sublist in cute_list]\n",
    "\n",
    "# convert the list of POS tags to a pandas series, add the pandas series to the existing data frame\n",
    "slayta.insert(5, 'correct_tags', final_list)\n",
    "\n",
    "# create a data frame with the words and tags grouped by utterance id\n",
    "slay_group = slayta.groupby('utterance_id').agg(lambda x: x.tolist())\n",
    "\n",
    "# count and print out the use of english and spanish\n",
    "slay_eng = (slayta[\"langid\"] == 'eng').sum()\n",
    "slay_spa = (slayta[\"langid\"] == 'spa').sum()\n",
    "\n",
    "# get a list of sentences\n",
    "sent_array = np.array(slay_group['surface'])\n",
    "sentences = [list(i) for i in sent_array]\n",
    "\n",
    "# get a list of tags\n",
    "tag_array = np.array(slay_group['correct_tags'])\n",
    "tags = [list(i) for i in tag_array]\n",
    "\n",
    "# get a list of langauges for the utterances\n",
    "lang_array = np.array(slay_group['langid'])\n",
    "languages = [list(i) for i in lang_array]\n",
    "\n",
    "# make a list of word bigrams at the utterance level \n",
    "new_sentences = []\n",
    "for i in sentences:\n",
    "    new_sentences.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a list of tag bigrams at the utterance level\n",
    "new_tags = []\n",
    "for i in tags:\n",
    "    new_tags.append(list(nltk.bigrams(i)))\n",
    "    \n",
    "new_languages = []\n",
    "for i in languages:\n",
    "    new_languages.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a merged list of the bigrams with their respective tags and language id\n",
    "merged_list = [tup_A + tup_B + tup_C for sublst_A, sublst_B, sublst_C in zip(new_sentences, new_tags, new_languages)\n",
    "               for tup_A, tup_B, tup_C in zip(sublst_A, sublst_B, sublst_C)]\n",
    "\n",
    "# get only the instances where is code switching by checking the language id \n",
    "code_switching = [item for item in merged_list if item[4] == 'spa' and item[5] == 'eng' \n",
    "                  or item[4] == 'eng' and item[5] == 'spa']\n",
    "\n",
    "# get only the PoS tags\n",
    "first_tag = [x[2] for x in code_switching]\n",
    "second_tag = [x[3] for x in code_switching]\n",
    "two_tags = list(zip(first_tag, second_tag))\n",
    "\n",
    "# get the most common tags \n",
    "c = Counter(tuple(i) for i in two_tags).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c539ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "slayta = pan.read_csv(\"herring4.csv\", sep = ',', encoding = \"unicode_escape\")\n",
    "\n",
    "# remove items in stopword list\n",
    "words = list(slayta ['surface'])\n",
    "\n",
    "# import stopwords lists\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "stop_words = [\"oh\", \"ok\", \"mmhm\", \"er\", \"ah\", \"ay\", \"um\", \"uhhuh\", \"wow\", \"huh\", \"\"]\n",
    "total = punct + stop_words\n",
    "\n",
    "super_slay = []\n",
    "for ele in words:\n",
    "    if ele in total:\n",
    "        super_slay.append('1')\n",
    "    else:\n",
    "        super_slay.append('0')\n",
    "        \n",
    "# add column to the pandas data frame of whether the word should not be included\n",
    "# the slayta.insert line has a note because it kept trying to add the same column\n",
    "# every time. If you are running for the first time, take out the note!\n",
    "inclusion = pan.Series(super_slay)\n",
    "slayta.insert(6, 'Inclusion', inclusion)\n",
    "options = ['0']\n",
    "slayta = slayta.loc[slayta['Inclusion'].isin(options)]\n",
    "\n",
    "# make a list of POS tags\n",
    "slayta = slayta[slayta.auto != \"unk\"]\n",
    "slayta['auto'] = slayta['auto'].fillna(\"NaN\")\n",
    "column_tags = list(slayta[\"auto\"])\n",
    "weird_tags = [ele.split(\",\") for ele in column_tags]\n",
    "\n",
    "# get the tags and get specific tags if encountered with contractions\n",
    "cute_list = [item.split(\".\") for sublist in weird_tags for item in sublist]\n",
    "final_list = [sublist[0] if len(sublist) <= 1\n",
    "          else 'N' if 'name' in sublist \n",
    "          else 'V' if 'SV' in sublist\n",
    "          else sublist[1] if 'V' not in sublist and '+BE' not in sublist \n",
    "          else 'V' if 'V' in sublist and \"PRON\" not in sublist\n",
    "          else sublist[1] + '+V' for sublist in cute_list]\n",
    "\n",
    "# convert the list of POS tags to a pandas series, add the pandas series to the existing data frame\n",
    "slayta.insert(5, 'correct_tags', final_list)\n",
    "\n",
    "# create a data frame with the words and tags grouped by utterance id\n",
    "slay_group = slayta.groupby('utterance_id').agg(lambda x: x.tolist())\n",
    "\n",
    "# count and print out the use of english and spanish\n",
    "slay_eng = (slayta[\"langid\"] == 'eng').sum()\n",
    "slay_spa = (slayta[\"langid\"] == 'spa').sum()\n",
    "\n",
    "# get a list of sentences\n",
    "sent_array = np.array(slay_group['surface'])\n",
    "sentences = [list(i) for i in sent_array]\n",
    "\n",
    "# get a list of tags\n",
    "tag_array = np.array(slay_group['correct_tags'])\n",
    "tags = [list(i) for i in tag_array]\n",
    "\n",
    "# get a list of langauges for the utterances\n",
    "lang_array = np.array(slay_group['langid'])\n",
    "languages = [list(i) for i in lang_array]\n",
    "\n",
    "# make a list of word bigrams at the utterance level \n",
    "new_sentences = []\n",
    "for i in sentences:\n",
    "    new_sentences.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a list of tag bigrams at the utterance level\n",
    "new_tags = []\n",
    "for i in tags:\n",
    "    new_tags.append(list(nltk.bigrams(i)))\n",
    "    \n",
    "new_languages = []\n",
    "for i in languages:\n",
    "    new_languages.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a merged list of the bigrams with their respective tags and language id\n",
    "merged_list = [tup_A + tup_B + tup_C for sublst_A, sublst_B, sublst_C in zip(new_sentences, new_tags, new_languages)\n",
    "               for tup_A, tup_B, tup_C in zip(sublst_A, sublst_B, sublst_C)]\n",
    "\n",
    "# get only the instances where is code switching by checking the language id \n",
    "code_switching = [item for item in merged_list if item[4] == 'spa' and item[5] == 'eng' \n",
    "                  or item[4] == 'eng' and item[5] == 'spa']\n",
    "\n",
    "# get only the PoS tags\n",
    "first_tag = [x[2] for x in code_switching]\n",
    "second_tag = [x[3] for x in code_switching]\n",
    "two_tags = list(zip(first_tag, second_tag))\n",
    "\n",
    "# get the most common tags \n",
    "d = Counter(tuple(i) for i in two_tags).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d8a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "slayta = pan.read_csv(\"herring5.csv\", sep = ',', encoding = \"unicode_escape\")\n",
    "\n",
    "# remove items in stopword list\n",
    "words = list(slayta ['surface'])\n",
    "\n",
    "# import stopwords lists\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "stop_words = [\"oh\", \"ok\", \"mmhm\", \"er\", \"ah\", \"ay\", \"um\", \"uhhuh\", \"wow\", \"huh\", \"\"]\n",
    "total = punct + stop_words\n",
    "\n",
    "super_slay = []\n",
    "for ele in words:\n",
    "    if ele in total:\n",
    "        super_slay.append('1')\n",
    "    else:\n",
    "        super_slay.append('0')\n",
    "        \n",
    "# add column to the pandas data frame of whether the word should not be included\n",
    "# the slayta.insert line has a note because it kept trying to add the same column\n",
    "# every time. If you are running for the first time, take out the note!\n",
    "inclusion = pan.Series(super_slay)\n",
    "slayta.insert(6, 'Inclusion', inclusion)\n",
    "options = ['0']\n",
    "slayta = slayta.loc[slayta['Inclusion'].isin(options)]\n",
    "\n",
    "# make a list of POS tags \n",
    "slayta = slayta[slayta.auto != \"unk\"]\n",
    "slayta['auto'] = slayta['auto'].fillna(\"NaN\")\n",
    "column_tags = list(slayta[\"auto\"])\n",
    "weird_tags = [ele.split(\",\") for ele in column_tags]\n",
    "\n",
    "# get the tags and get specific tags if encountered with contractions\n",
    "cute_list = [item.split(\".\") for sublist in weird_tags for item in sublist]\n",
    "final_list = [sublist[0] if len(sublist) <= 1\n",
    "          else 'N' if 'name' in sublist \n",
    "          else 'V' if 'SV' in sublist\n",
    "          else sublist[1] if 'V' not in sublist and '+BE' not in sublist \n",
    "          else 'V' if 'V' in sublist and \"PRON\" not in sublist\n",
    "          else sublist[1] + '+V' for sublist in cute_list]\n",
    "\n",
    "# convert the list of POS tags to a pandas series, add the pandas series to the existing data frame\n",
    "slayta.insert(5, 'correct_tags', final_list)\n",
    "\n",
    "# create a data frame with the words and tags grouped by utterance id\n",
    "slay_group = slayta.groupby('utterance_id').agg(lambda x: x.tolist())\n",
    "\n",
    "# count and print out the use of english and spanish\n",
    "slay_eng = (slayta[\"langid\"] == 'eng').sum()\n",
    "slay_spa = (slayta[\"langid\"] == 'spa').sum()\n",
    "\n",
    "# get a list of sentences\n",
    "sent_array = np.array(slay_group['surface'])\n",
    "sentences = [list(i) for i in sent_array]\n",
    "\n",
    "# get a list of tags\n",
    "tag_array = np.array(slay_group['correct_tags'])\n",
    "tags = [list(i) for i in tag_array]\n",
    "\n",
    "# get a list of langauges for the utterances\n",
    "lang_array = np.array(slay_group['langid'])\n",
    "languages = [list(i) for i in lang_array]\n",
    "\n",
    "# make a list of word bigrams at the utterance level \n",
    "new_sentences = []\n",
    "for i in sentences:\n",
    "    new_sentences.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a list of tag bigrams at the utterance level\n",
    "new_tags = []\n",
    "for i in tags:\n",
    "    new_tags.append(list(nltk.bigrams(i)))\n",
    "    \n",
    "new_languages = []\n",
    "for i in languages:\n",
    "    new_languages.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a merged list of the bigrams with their respective tags and language id\n",
    "merged_list = [tup_A + tup_B + tup_C for sublst_A, sublst_B, sublst_C in zip(new_sentences, new_tags, new_languages)\n",
    "               for tup_A, tup_B, tup_C in zip(sublst_A, sublst_B, sublst_C)]\n",
    "\n",
    "# get only the instances where is code switching by checking the language id \n",
    "code_switching = [item for item in merged_list if item[4] == 'spa' and item[5] == 'eng' \n",
    "                  or item[4] == 'eng' and item[5] == 'spa']\n",
    "\n",
    "# get only the PoS tags\n",
    "first_tag = [x[2] for x in code_switching]\n",
    "second_tag = [x[3] for x in code_switching]\n",
    "two_tags = list(zip(first_tag, second_tag))\n",
    "\n",
    "# get the most common tags \n",
    "e = Counter(tuple(i) for i in two_tags).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2e45f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "slayta = pan.read_csv(\"herring6.csv\", sep = ',', encoding = \"unicode_escape\")\n",
    "\n",
    "# remove items in stopword list\n",
    "words = list(slayta ['surface'])\n",
    "\n",
    "# import stopwords lists\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "stop_words = [\"oh\", \"ok\", \"mmhm\", \"er\", \"ah\", \"ay\", \"um\", \"uhhuh\", \"wow\", \"huh\", \"\"]\n",
    "total = punct + stop_words\n",
    "\n",
    "super_slay = []\n",
    "for ele in words:\n",
    "    if ele in total:\n",
    "        super_slay.append('1')\n",
    "    else:\n",
    "        super_slay.append('0')\n",
    "        \n",
    "# add column to the pandas data frame of whether the word should not be included\n",
    "# the slayta.insert line has a note because it kept trying to add the same column\n",
    "# every time. If you are running for the first time, take out the note!\n",
    "inclusion = pan.Series(super_slay)\n",
    "slayta.insert(6, 'Inclusion', inclusion)\n",
    "options = ['0']\n",
    "slayta = slayta.loc[slayta['Inclusion'].isin(options)]\n",
    "\n",
    "# make a list of POS tags \n",
    "slayta = slayta[slayta.auto != \"unk\"]\n",
    "slayta['auto'] = slayta['auto'].fillna(\"NaN\")\n",
    "column_tags = list(slayta[\"auto\"])\n",
    "weird_tags = [ele.split(\",\") for ele in column_tags]\n",
    "\n",
    "# get the tags and get specific tags if encountered with contractions\n",
    "cute_list = [item.split(\".\") for sublist in weird_tags for item in sublist]\n",
    "final_list = [sublist[0] if len(sublist) <= 1\n",
    "          else 'N' if 'name' in sublist \n",
    "          else 'V' if 'SV' in sublist\n",
    "          else sublist[1] if 'V' not in sublist and '+BE' not in sublist \n",
    "          else 'V' if 'V' in sublist and \"PRON\" not in sublist\n",
    "          else sublist[1] + '+V' for sublist in cute_list]\n",
    "\n",
    "# convert the list of POS tags to a pandas series, add the pandas series to the existing data frame\n",
    "slayta.insert(5, 'correct_tags', final_list)\n",
    "\n",
    "# create a data frame with the words and tags grouped by utterance id\n",
    "slay_group = slayta.groupby('utterance_id').agg(lambda x: x.tolist())\n",
    "\n",
    "# count and print out the use of english and spanish\n",
    "slay_eng = (slayta[\"langid\"] == 'eng').sum()\n",
    "slay_spa = (slayta[\"langid\"] == 'spa').sum()\n",
    "\n",
    "# get a list of sentences\n",
    "sent_array = np.array(slay_group['surface'])\n",
    "sentences = [list(i) for i in sent_array]\n",
    "\n",
    "# get a list of tags\n",
    "tag_array = np.array(slay_group['correct_tags'])\n",
    "tags = [list(i) for i in tag_array]\n",
    "\n",
    "# get a list of langauges for the utterances\n",
    "lang_array = np.array(slay_group['langid'])\n",
    "languages = [list(i) for i in lang_array]\n",
    "\n",
    "# make a list of word bigrams at the utterance level \n",
    "new_sentences = []\n",
    "for i in sentences:\n",
    "    new_sentences.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a list of tag bigrams at the utterance level\n",
    "new_tags = []\n",
    "for i in tags:\n",
    "    new_tags.append(list(nltk.bigrams(i)))\n",
    "    \n",
    "new_languages = []\n",
    "for i in languages:\n",
    "    new_languages.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a merged list of the bigrams with their respective tags and language id\n",
    "merged_list = [tup_A + tup_B + tup_C for sublst_A, sublst_B, sublst_C in zip(new_sentences, new_tags, new_languages)\n",
    "               for tup_A, tup_B, tup_C in zip(sublst_A, sublst_B, sublst_C)]\n",
    "\n",
    "# get only the instances where is code switching by checking the language id \n",
    "code_switching = [item for item in merged_list if item[4] == 'spa' and item[5] == 'eng' \n",
    "                  or item[4] == 'eng' and item[5] == 'spa']\n",
    "\n",
    "# get only the PoS tags\n",
    "first_tag = [x[2] for x in code_switching]\n",
    "second_tag = [x[3] for x in code_switching]\n",
    "two_tags = list(zip(first_tag, second_tag))\n",
    "\n",
    "# get the most common tags \n",
    "f = Counter(tuple(i) for i in two_tags).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95cd3050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "slayta = pan.read_csv(\"herring7.csv\", sep = ',', encoding = \"unicode_escape\")\n",
    "\n",
    "# remove items in stopword list\n",
    "words = list(slayta ['surface'])\n",
    "\n",
    "# import stopwords lists\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "stop_words = [\"oh\", \"ok\", \"mmhm\", \"er\", \"ah\", \"ay\", \"um\", \"uhhuh\", \"wow\", \"huh\", \"\"]\n",
    "total = punct + stop_words\n",
    "\n",
    "super_slay = []\n",
    "for ele in words:\n",
    "    if ele in total:\n",
    "        super_slay.append('1')\n",
    "    else:\n",
    "        super_slay.append('0')\n",
    "        \n",
    "# add column to the pandas data frame of whether the word should not be included\n",
    "# the slayta.insert line has a note because it kept trying to add the same column\n",
    "# every time. If you are running for the first time, take out the note!\n",
    "inclusion = pan.Series(super_slay)\n",
    "slayta.insert(6, 'Inclusion', inclusion)\n",
    "options = ['0']\n",
    "slayta = slayta.loc[slayta['Inclusion'].isin(options)]\n",
    "\n",
    "# make a list of POS tags \n",
    "slayta = slayta[slayta.auto != \"unk\"]\n",
    "slayta['auto'] = slayta['auto'].fillna(\"NaN\")\n",
    "column_tags = list(slayta[\"auto\"])\n",
    "weird_tags = [ele.split(\",\") for ele in column_tags]\n",
    "\n",
    "# get the tags and get specific tags if encountered with contractions\n",
    "cute_list = [item.split(\".\") for sublist in weird_tags for item in sublist]\n",
    "final_list = [sublist[0] if len(sublist) <= 1\n",
    "          else 'N' if 'name' in sublist \n",
    "          else 'V' if 'SV' in sublist\n",
    "          else sublist[1] if 'V' not in sublist and '+BE' not in sublist \n",
    "          else 'V' if 'V' in sublist and \"PRON\" not in sublist\n",
    "          else sublist[1] + '+V' for sublist in cute_list]\n",
    "\n",
    "# convert the list of POS tags to a pandas series, add the pandas series to the existing data frame\n",
    "slayta.insert(5, 'correct_tags', final_list)\n",
    "\n",
    "# create a data frame with the words and tags grouped by utterance id\n",
    "slay_group = slayta.groupby('utterance_id').agg(lambda x: x.tolist())\n",
    "\n",
    "# count and print out the use of english and spanish\n",
    "slay_eng = (slayta[\"langid\"] == 'eng').sum()\n",
    "slay_spa = (slayta[\"langid\"] == 'spa').sum()\n",
    "\n",
    "# get a list of sentences\n",
    "sent_array = np.array(slay_group['surface'])\n",
    "sentences = [list(i) for i in sent_array]\n",
    "\n",
    "# get a list of tags\n",
    "tag_array = np.array(slay_group['correct_tags'])\n",
    "tags = [list(i) for i in tag_array]\n",
    "\n",
    "# get a list of langauges for the utterances\n",
    "lang_array = np.array(slay_group['langid'])\n",
    "languages = [list(i) for i in lang_array]\n",
    "\n",
    "# make a list of word bigrams at the utterance level \n",
    "new_sentences = []\n",
    "for i in sentences:\n",
    "    new_sentences.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a list of tag bigrams at the utterance level\n",
    "new_tags = []\n",
    "for i in tags:\n",
    "    new_tags.append(list(nltk.bigrams(i)))\n",
    "    \n",
    "new_languages = []\n",
    "for i in languages:\n",
    "    new_languages.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a merged list of the bigrams with their respective tags and language id\n",
    "merged_list = [tup_A + tup_B + tup_C for sublst_A, sublst_B, sublst_C in zip(new_sentences, new_tags, new_languages)\n",
    "               for tup_A, tup_B, tup_C in zip(sublst_A, sublst_B, sublst_C)]\n",
    "\n",
    "# get only the instances where is code switching by checking the language id \n",
    "code_switching = [item for item in merged_list if item[4] == 'spa' and item[5] == 'eng' \n",
    "                  or item[4] == 'eng' and item[5] == 'spa']\n",
    "\n",
    "# get only the PoS tags\n",
    "first_tag = [x[2] for x in code_switching]\n",
    "second_tag = [x[3] for x in code_switching]\n",
    "two_tags = list(zip(first_tag, second_tag))\n",
    "\n",
    "# get the most common tags \n",
    "g = Counter(tuple(i) for i in two_tags).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f177b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "slayta = pan.read_csv(\"herring8.csv\", sep = ',', encoding = \"unicode_escape\")\n",
    "\n",
    "# remove items in stopword list\n",
    "words = list(slayta ['surface'])\n",
    "\n",
    "# import stopwords lists\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "stop_words = [\"oh\", \"ok\", \"mmhm\", \"er\", \"ah\", \"ay\", \"um\", \"uhhuh\", \"wow\", \"huh\", \"\"]\n",
    "total = punct + stop_words\n",
    "\n",
    "super_slay = []\n",
    "for ele in words:\n",
    "    if ele in total:\n",
    "        super_slay.append('1')\n",
    "    else:\n",
    "        super_slay.append('0')\n",
    "        \n",
    "# add column to the pandas data frame of whether the word should not be included\n",
    "# the slayta.insert line has a note because it kept trying to add the same column\n",
    "# every time. If you are running for the first time, take out the note!\n",
    "inclusion = pan.Series(super_slay)\n",
    "slayta.insert(6, 'Inclusion', inclusion)\n",
    "options = ['0']\n",
    "slayta = slayta.loc[slayta['Inclusion'].isin(options)]\n",
    "\n",
    "# make a list of POS tags \n",
    "slayta = slayta[slayta.auto != \"unk\"]\n",
    "slayta['auto'] = slayta['auto'].fillna(\"NaN\")\n",
    "column_tags = list(slayta[\"auto\"])\n",
    "weird_tags = [ele.split(\",\") for ele in column_tags]\n",
    "\n",
    "# get the tags and get specific tags if encountered with contractions\n",
    "cute_list = [item.split(\".\") for sublist in weird_tags for item in sublist]\n",
    "final_list = [sublist[0] if len(sublist) <= 1\n",
    "          else 'N' if 'name' in sublist \n",
    "          else 'V' if 'SV' in sublist\n",
    "          else sublist[1] if 'V' not in sublist and '+BE' not in sublist \n",
    "          else 'V' if 'V' in sublist and \"PRON\" not in sublist\n",
    "          else sublist[1] + '+V' for sublist in cute_list]\n",
    "\n",
    "# convert the list of POS tags to a pandas series, add the pandas series to the existing data frame\n",
    "slayta.insert(5, 'correct_tags', final_list)\n",
    "\n",
    "# create a data frame with the words and tags grouped by utterance id\n",
    "slay_group = slayta.groupby('utterance_id').agg(lambda x: x.tolist())\n",
    "\n",
    "# count and print out the use of english and spanish\n",
    "slay_eng = (slayta[\"langid\"] == 'eng').sum()\n",
    "slay_spa = (slayta[\"langid\"] == 'spa').sum()\n",
    "\n",
    "# get a list of sentences\n",
    "sent_array = np.array(slay_group['surface'])\n",
    "sentences = [list(i) for i in sent_array]\n",
    "\n",
    "# get a list of tags\n",
    "tag_array = np.array(slay_group['correct_tags'])\n",
    "tags = [list(i) for i in tag_array]\n",
    "\n",
    "# get a list of langauges for the utterances\n",
    "lang_array = np.array(slay_group['langid'])\n",
    "languages = [list(i) for i in lang_array]\n",
    "\n",
    "# make a list of word bigrams at the utterance level \n",
    "new_sentences = []\n",
    "for i in sentences:\n",
    "    new_sentences.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a list of tag bigrams at the utterance level\n",
    "new_tags = []\n",
    "for i in tags:\n",
    "    new_tags.append(list(nltk.bigrams(i)))\n",
    "    \n",
    "new_languages = []\n",
    "for i in languages:\n",
    "    new_languages.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a merged list of the bigrams with their respective tags and language id\n",
    "merged_list = [tup_A + tup_B + tup_C for sublst_A, sublst_B, sublst_C in zip(new_sentences, new_tags, new_languages)\n",
    "               for tup_A, tup_B, tup_C in zip(sublst_A, sublst_B, sublst_C)]\n",
    "\n",
    "# get only the instances where is code switching by checking the language id \n",
    "code_switching = [item for item in merged_list if item[4] == 'spa' and item[5] == 'eng' \n",
    "                  or item[4] == 'eng' and item[5] == 'spa']\n",
    "\n",
    "# get only the PoS tags\n",
    "first_tag = [x[2] for x in code_switching]\n",
    "second_tag = [x[3] for x in code_switching]\n",
    "two_tags = list(zip(first_tag, second_tag))\n",
    "\n",
    "# get the most common tags \n",
    "h = Counter(tuple(i) for i in two_tags).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d25eaca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "slayta = pan.read_csv(\"herring9.csv\", sep = ',', encoding = \"unicode_escape\")\n",
    "\n",
    "# remove items in stopword list\n",
    "words = list(slayta ['surface'])\n",
    "\n",
    "# import stopwords lists\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "stop_words = [\"oh\", \"ok\", \"mmhm\", \"er\", \"ah\", \"ay\", \"um\", \"uhhuh\", \"wow\", \"huh\", \"\"]\n",
    "total = punct + stop_words\n",
    "\n",
    "super_slay = []\n",
    "for ele in words:\n",
    "    if ele in total:\n",
    "        super_slay.append('1')\n",
    "    else:\n",
    "        super_slay.append('0')\n",
    "        \n",
    "# add column to the pandas data frame of whether the word should not be included\n",
    "# the slayta.insert line has a note because it kept trying to add the same column\n",
    "# every time. If you are running for the first time, take out the note!\n",
    "inclusion = pan.Series(super_slay)\n",
    "slayta.insert(6, 'Inclusion', inclusion)\n",
    "options = ['0']\n",
    "slayta = slayta.loc[slayta['Inclusion'].isin(options)]\n",
    "\n",
    "# make a list of POS tags \n",
    "slayta = slayta[slayta.auto != \"unk\"]\n",
    "slayta['auto'] = slayta['auto'].fillna(\"NaN\")\n",
    "column_tags = list(slayta[\"auto\"])\n",
    "weird_tags = [ele.split(\",\") for ele in column_tags]\n",
    "\n",
    "# get the tags and get specific tags if encountered with contractions\n",
    "cute_list = [item.split(\".\") for sublist in weird_tags for item in sublist]\n",
    "final_list = [sublist[0] if len(sublist) <= 1\n",
    "          else 'N' if 'name' in sublist \n",
    "          else 'V' if 'SV' in sublist\n",
    "          else sublist[1] if 'V' not in sublist and '+BE' not in sublist \n",
    "          else 'V' if 'V' in sublist and \"PRON\" not in sublist\n",
    "          else sublist[1] + '+V' for sublist in cute_list]\n",
    "\n",
    "# convert the list of POS tags to a pandas series, add the pandas series to the existing data frame\n",
    "slayta.insert(5, 'correct_tags', final_list)\n",
    "\n",
    "# create a data frame with the words and tags grouped by utterance id\n",
    "slay_group = slayta.groupby('utterance_id').agg(lambda x: x.tolist())\n",
    "\n",
    "# count and print out the use of english and spanish\n",
    "slay_eng = (slayta[\"langid\"] == 'eng').sum()\n",
    "slay_spa = (slayta[\"langid\"] == 'spa').sum()\n",
    "\n",
    "# get a list of sentences\n",
    "sent_array = np.array(slay_group['surface'])\n",
    "sentences = [list(i) for i in sent_array]\n",
    "\n",
    "# get a list of tags\n",
    "tag_array = np.array(slay_group['correct_tags'])\n",
    "tags = [list(i) for i in tag_array]\n",
    "\n",
    "# get a list of langauges for the utterances\n",
    "lang_array = np.array(slay_group['langid'])\n",
    "languages = [list(i) for i in lang_array]\n",
    "\n",
    "# make a list of word bigrams at the utterance level \n",
    "new_sentences = []\n",
    "for i in sentences:\n",
    "    new_sentences.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a list of tag bigrams at the utterance level\n",
    "new_tags = []\n",
    "for i in tags:\n",
    "    new_tags.append(list(nltk.bigrams(i)))\n",
    "    \n",
    "new_languages = []\n",
    "for i in languages:\n",
    "    new_languages.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a merged list of the bigrams with their respective tags and language id\n",
    "merged_list = [tup_A + tup_B + tup_C for sublst_A, sublst_B, sublst_C in zip(new_sentences, new_tags, new_languages)\n",
    "               for tup_A, tup_B, tup_C in zip(sublst_A, sublst_B, sublst_C)]\n",
    "\n",
    "# get only the instances where is code switching by checking the language id \n",
    "code_switching = [item for item in merged_list if item[4] == 'spa' and item[5] == 'eng' \n",
    "                  or item[4] == 'eng' and item[5] == 'spa']\n",
    "\n",
    "# get only the PoS tags\n",
    "first_tag = [x[2] for x in code_switching]\n",
    "second_tag = [x[3] for x in code_switching]\n",
    "two_tags = list(zip(first_tag, second_tag))\n",
    "\n",
    "# get the most common tags \n",
    "i = Counter(tuple(i) for i in two_tags).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3cada69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "slayta = pan.read_csv(\"herring10.csv\", sep = ',', encoding = \"unicode_escape\")\n",
    "\n",
    "# remove items in stopword list\n",
    "words = list(slayta ['surface'])\n",
    "\n",
    "# import stopwords lists\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "stop_words = [\"oh\", \"ok\", \"mmhm\", \"er\", \"ah\", \"ay\", \"um\", \"uhhuh\", \"wow\", \"huh\", \"\"]\n",
    "total = punct + stop_words\n",
    "\n",
    "super_slay = []\n",
    "for ele in words:\n",
    "    if ele in total:\n",
    "        super_slay.append('1')\n",
    "    else:\n",
    "        super_slay.append('0')\n",
    "        \n",
    "# add column to the pandas data frame of whether the word should not be included\n",
    "# the slayta.insert line has a note because it kept trying to add the same column\n",
    "# every time. If you are running for the first time, take out the note!\n",
    "inclusion = pan.Series(super_slay)\n",
    "slayta.insert(6, 'Inclusion', inclusion)\n",
    "options = ['0']\n",
    "slayta = slayta.loc[slayta['Inclusion'].isin(options)]\n",
    "\n",
    "# make a list of POS tags \n",
    "slayta = slayta[slayta.auto != \"unk\"]\n",
    "slayta['auto'] = slayta['auto'].fillna(\"NaN\")\n",
    "column_tags = list(slayta[\"auto\"])\n",
    "weird_tags = [ele.split(\",\") for ele in column_tags]\n",
    "\n",
    "# get the tags and get specific tags if encountered with contractions\n",
    "cute_list = [item.split(\".\") for sublist in weird_tags for item in sublist]\n",
    "final_list = [sublist[0] if len(sublist) <= 1\n",
    "          else 'N' if 'name' in sublist \n",
    "          else 'V' if 'SV' in sublist\n",
    "          else sublist[1] if 'V' not in sublist and '+BE' not in sublist \n",
    "          else 'V' if 'V' in sublist and \"PRON\" not in sublist\n",
    "          else sublist[1] + '+V' for sublist in cute_list]\n",
    "\n",
    "# convert the list of POS tags to a pandas series, add the pandas series to the existing data frame\n",
    "slayta.insert(5, 'correct_tags', final_list)\n",
    "\n",
    "# create a data frame with the words and tags grouped by utterance id\n",
    "slay_group = slayta.groupby('utterance_id').agg(lambda x: x.tolist())\n",
    "\n",
    "# count and print out the use of english and spanish\n",
    "slay_eng = (slayta[\"langid\"] == 'eng').sum()\n",
    "slay_spa = (slayta[\"langid\"] == 'spa').sum()\n",
    "\n",
    "# get a list of sentences\n",
    "sent_array = np.array(slay_group['surface'])\n",
    "sentences = [list(i) for i in sent_array]\n",
    "\n",
    "# get a list of tags\n",
    "tag_array = np.array(slay_group['correct_tags'])\n",
    "tags = [list(i) for i in tag_array]\n",
    "\n",
    "# get a list of langauges for the utterances\n",
    "lang_array = np.array(slay_group['langid'])\n",
    "languages = [list(i) for i in lang_array]\n",
    "\n",
    "# make a list of word bigrams at the utterance level \n",
    "new_sentences = []\n",
    "for i in sentences:\n",
    "    new_sentences.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a list of tag bigrams at the utterance level\n",
    "new_tags = []\n",
    "for i in tags:\n",
    "    new_tags.append(list(nltk.bigrams(i)))\n",
    "    \n",
    "new_languages = []\n",
    "for i in languages:\n",
    "    new_languages.append(list(nltk.bigrams(i)))\n",
    "\n",
    "# make a merged list of the bigrams with their respective tags and language id\n",
    "merged_list = [tup_A + tup_B + tup_C for sublst_A, sublst_B, sublst_C in zip(new_sentences, new_tags, new_languages)\n",
    "               for tup_A, tup_B, tup_C in zip(sublst_A, sublst_B, sublst_C)]\n",
    "\n",
    "# get only the instances where is code switching by checking the language id \n",
    "code_switching = [item for item in merged_list if item[4] == 'spa' and item[5] == 'eng' \n",
    "                  or item[4] == 'eng' and item[5] == 'spa']\n",
    "\n",
    "# get only the PoS tags\n",
    "first_tag = [x[2] for x in code_switching]\n",
    "second_tag = [x[3] for x in code_switching]\n",
    "two_tags = list(zip(first_tag, second_tag))\n",
    "\n",
    "# get the most common tags \n",
    "j = Counter(tuple(i) for i in two_tags).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5024e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results = (a + b + c + d + e + f + g + h + i + j)\n",
    "\n",
    "grouped_counters = {}\n",
    "for counter in grouped_results:\n",
    "    key = counter[0]\n",
    "    value = counter[1]\n",
    "    if key in grouped_counters:\n",
    "        grouped_counters[key] += value\n",
    "    else:\n",
    "        grouped_counters[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "108af6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('N', 'CONJ'): 18,\n",
       " ('N', 'PRON'): 16,\n",
       " ('ADV', 'PRON'): 15,\n",
       " ('V', 'PRON'): 25,\n",
       " ('V', 'V'): 13,\n",
       " ('ADJ', 'PREP'): 6,\n",
       " ('V', 'ADV'): 12,\n",
       " ('ADJ', 'CONJ'): 4,\n",
       " ('CONJ', 'PRON'): 12,\n",
       " ('CONJ', 'DET'): 7,\n",
       " ('CONJ', 'PRON+V'): 2,\n",
       " ('N', 'N'): 4,\n",
       " ('CONJ', 'V'): 6,\n",
       " ('ADV', 'ADJ'): 3,\n",
       " ('ADJ', 'PRON'): 2,\n",
       " ('PRON', 'PRON'): 4,\n",
       " ('N', 'ADJ'): 4,\n",
       " ('ADJ', 'N'): 8,\n",
       " ('V', 'PRON+V'): 2,\n",
       " ('PRON+V', 'PRON'): 1,\n",
       " ('CONJ', 'DEM'): 2,\n",
       " ('DET', 'N'): 35,\n",
       " ('PRON', 'V'): 3,\n",
       " ('CONJ', 'REL+GB'): 1,\n",
       " ('V', 'PREP'): 13,\n",
       " ('V', 'CONJ'): 12,\n",
       " ('PRON', 'ADV'): 2,\n",
       " ('CONJ', 'ADV'): 5,\n",
       " ('N', 'PREP'): 10,\n",
       " ('CONJ', 'CONJ'): 8,\n",
       " ('ADV', 'N'): 1,\n",
       " ('N', 'DEM'): 1,\n",
       " ('V', 'DET'): 6,\n",
       " ('CONJ', 'PREP'): 3,\n",
       " ('PREP', 'DET'): 2,\n",
       " ('V', 'N'): 19,\n",
       " ('ADV', 'V'): 6,\n",
       " ('N', 'V'): 10,\n",
       " ('ADJ', 'ADV'): 2,\n",
       " ('PREP', 'N'): 11,\n",
       " ('CONJ', 'N'): 6,\n",
       " ('PREP+DET', 'N'): 4,\n",
       " ('CONJ', 'REL'): 1,\n",
       " ('ADJ+ADV', 'PREP'): 1,\n",
       " ('PRON', 'CONJ'): 3,\n",
       " ('PRON', 'N'): 2,\n",
       " ('ADJ', 'V'): 2,\n",
       " ('ADV', 'ADV'): 3,\n",
       " ('N', 'IM'): 1,\n",
       " ('IM', 'CONJ'): 2,\n",
       " ('ADJ', 'PRON+V'): 2,\n",
       " ('PRON+V', 'ADJ'): 1,\n",
       " ('PREP', 'V'): 3,\n",
       " ('V', 'name'): 2,\n",
       " ('N', 'PRON+V'): 1,\n",
       " ('PRON', 'PREP'): 2,\n",
       " ('PREP', 'ADJ'): 4,\n",
       " ('N', 'ADV'): 5,\n",
       " ('ADV', 'PREP'): 3,\n",
       " ('ADJ', 'ADJ'): 2,\n",
       " ('N', 'NUM'): 1,\n",
       " ('V', 'ADJ'): 8,\n",
       " ('ADV', 'REL'): 1,\n",
       " ('PREP', 'NUM'): 2,\n",
       " ('PREP+DET', 'DET'): 1,\n",
       " ('NUM', 'N'): 1,\n",
       " ('V', 'ADJ+ADV'): 1,\n",
       " ('PREP', 'name'): 7,\n",
       " ('DET', 'ADJ+PV'): 2,\n",
       " ('CONJ', 'name'): 1,\n",
       " ('DET', 'ADJ'): 3,\n",
       " ('CONJ', 'AV'): 1,\n",
       " ('INT', 'N'): 1,\n",
       " ('ADV', 'PRON+V'): 1,\n",
       " ('AV', 'DET'): 1,\n",
       " ('PRON', 'DET'): 1,\n",
       " ('DEM', 'PRON'): 1,\n",
       " ('N', 'PREP+DET'): 2,\n",
       " ('REL', 'CONJ'): 1,\n",
       " ('ADV', 'AV'): 1,\n",
       " ('E', 'ADV'): 1,\n",
       " ('N', 'DET'): 3,\n",
       " ('ADV', 'CONJ'): 3,\n",
       " ('CONJ', 'ADJ+ADV'): 1,\n",
       " ('DET', 'ADV'): 1,\n",
       " ('V', 'NUM'): 3,\n",
       " ('DET', 'CONJ'): 1,\n",
       " ('ADJ', 'DET'): 1,\n",
       " ('V', 'ADV+PV'): 1,\n",
       " ('DET', 'DEM'): 2,\n",
       " ('E', 'V'): 1,\n",
       " ('DET', 'DET'): 1,\n",
       " ('DEM', 'V'): 1,\n",
       " ('PREP', 'DEM'): 1,\n",
       " ('CONJ', 'ADJ'): 2,\n",
       " ('INT', 'PREP'): 1,\n",
       " ('V', 'E'): 1,\n",
       " ('E', 'CONJ'): 1,\n",
       " ('name', 'DET'): 1,\n",
       " ('NUM', 'PREP'): 1,\n",
       " ('PREP', 'PREP'): 1,\n",
       " 's': 'ppppppp',\n",
       " 'e': 'nnnn',\n",
       " ('ADJ+ADV', 'V'): 1,\n",
       " ('ADJ+ADV', 'PRON'): 1,\n",
       " ('name', 'PRON'): 1,\n",
       " ('name', 'PREP'): 1,\n",
       " ('PRON+V', 'V'): 1,\n",
       " ('name', 'ADV'): 1,\n",
       " ('DET', 'name'): 1,\n",
       " ('name', 'CONJ'): 1,\n",
       " ('PRON', 'DEM'): 1,\n",
       " ('V', 'INT'): 1,\n",
       " ('DET', 'ADJ+ADV'): 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e050c86f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
